{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fe3e29",
   "metadata": {},
   "source": [
    "\n",
    "# Traffic Accident Analysis â€” Data Cleaning, EDA & Hotspot Visualization\n",
    "\n",
    "This notebook performs data loading, full data cleaning, exploratory data analysis (EDA), and hotspot visualization for a traffic accident dataset.\n",
    "It focuses on identifying patterns related to **road condition**, **weather**, and **time of day**, and visualizes accident hotspots and contributing factors.\n",
    "\n",
    "**How to use**\n",
    "1. Place your accident dataset CSV (for example, `US_Accidents_Dec20.csv` or `accidents.csv`) in the same folder as this notebook, **or**\n",
    "2. The notebook will attempt to download a public sample if available. If the dataset requires Kaggle authentication, upload it to the notebook environment or mount Google Drive in Colab.\n",
    "3. Run cells step-by-step in Jupyter, VS Code (Jupyter), or Google Colab.\n",
    "\n",
    "**Outputs**\n",
    "- Cleaned dataset saved as `accidents_cleaned.csv`\n",
    "- EDA plots (time-of-day, weather, road condition distributions)\n",
    "- Spatial hotspot visualizations (using folium if lat/lng provided)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7888fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Optional packages (uncomment to install in Colab)\n",
    "# !pip install folium\n",
    "# !pip install geopandas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142319dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load dataset (local or attempt known sources) ===\n",
    "local_candidates = [\n",
    "    \"US_Accidents_Dec20.csv\",\n",
    "    \"US_Accidents_Dec21.csv\",\n",
    "    \"US_Accidents.csv\",\n",
    "    \"accidents.csv\",\n",
    "    \"accidents_2016_2020.csv\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in local_candidates:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            print(f\"Loaded local file: {p}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Found file {p} but failed to read: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    print(\"No local file found. Please upload your CSV to the environment or mount Google Drive.\")\n",
    "    # Optionally try a small public sample (if available)\n",
    "    sample_url = \"https://raw.githubusercontent.com/plotly/datasets/master/2016-weather-data-seattle.csv\"\n",
    "    try:\n",
    "        print(\"Attempting to download a small sample (not actual accidents data) for demonstration...\")\n",
    "        df = pd.read_csv(sample_url)\n",
    "        print(\"Downloaded sample dataset (weather sample). NOTE: Replace with actual accident CSV for real analysis.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"No dataset available. Please upload your accident CSV file.\") from e\n",
    "\n",
    "print('\\nDataset shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77badf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Data Cleaning (adaptive) ===\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "\n",
    "# Try to identify typical columns if present\n",
    "possible_time_cols = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower()]\n",
    "possible_lat = [c for c in df.columns if 'lat' in c.lower()]\n",
    "possible_lng = [c for c in df.columns if 'lon' in c.lower() or 'lng' in c.lower()]\n",
    "\n",
    "print(\"Detected time columns:\", possible_time_cols)\n",
    "print(\"Detected latitude columns:\", possible_lat)\n",
    "print(\"Detected longitude columns:\", possible_lng)\n",
    "\n",
    "# Parse Start_Time or equivalent into datetime, hour, day, month\n",
    "time_col = None\n",
    "for c in possible_time_cols:\n",
    "    lc = c.lower()\n",
    "    if 'start' in lc or 'time' in lc or 'date' in lc:\n",
    "        time_col = c; break\n",
    "\n",
    "if time_col is not None:\n",
    "    print(\"Using time column:\", time_col)\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "    df['hour'] = df[time_col].dt.hour\n",
    "    df['day_of_week'] = df[time_col].dt.day_name()\n",
    "    df['month'] = df[time_col].dt.month_name()\n",
    "else:\n",
    "    print(\"No obvious time column found. You'll need to provide a time column for temporal analysis.\")\n",
    "\n",
    "# Standardize weather/road condition columns if present\n",
    "weather_cols = [c for c in df.columns if 'weather' in c.lower()]\n",
    "road_cols = [c for c in df.columns if 'road' in c.lower() or 'surface' in c.lower()]\n",
    "\n",
    "print(\"Weather-like cols:\", weather_cols)\n",
    "print(\"Road-like cols:\", road_cols)\n",
    "\n",
    "# Clean weather column: lower, strip, group rare categories\n",
    "if weather_cols:\n",
    "    wc = weather_cols[0]\n",
    "    df[wc] = df[wc].astype(str).str.lower().str.strip()\n",
    "    # simplify categories (basic rules)\n",
    "    df['weather_simple'] = df[wc].replace({\n",
    "        'clear':'clear', 'sunny':'clear', 'mostly clear':'clear',\n",
    "        'rain':'rain', 'light rain':'rain', 'drizzle':'rain', 'heavy rain':'rain',\n",
    "        'snow':'snow', 'light snow':'snow', 'sleet':'snow',\n",
    "        'fog':'fog', 'haze':'fog', 'mist':'fog', 'overcast':'cloudy', 'cloudy':'cloudy'\n",
    "    })\n",
    "else:\n",
    "    df['weather_simple'] = np.nan\n",
    "\n",
    "# Clean road condition column\n",
    "if road_cols:\n",
    "    rc = road_cols[0]\n",
    "    df[rc] = df[rc].astype(str).str.lower().str.strip()\n",
    "    df['road_simple'] = df[rc].replace({\n",
    "        'dry':'dry', 'wet':'wet', 'icy':'icy', 'snow':'snow', 'damp':'wet'\n",
    "    })\n",
    "else:\n",
    "    df['road_simple'] = np.nan\n",
    "\n",
    "# If lat/lng present, drop rows with missing coordinates for hotspot maps\n",
    "lat_col = possible_lat[0] if possible_lat else None\n",
    "lng_col = possible_lng[0] if possible_lng else None\n",
    "\n",
    "if lat_col and lng_col:\n",
    "    df = df.dropna(subset=[lat_col, lng_col])\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')\n",
    "    df[lng_col] = pd.to_numeric(df[lng_col], errors='coerce')\n",
    "\n",
    "# Basic missing value report\n",
    "print(\"\\nMissing values summary:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# Save cleaned CSV\n",
    "cleaned_path = \"accidents_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print(f\"Saved cleaned dataset to: {cleaned_path}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Exploratory Data Analysis (EDA) ===\n",
    "# Time of day distribution\n",
    "if 'hour' in df.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(x='hour', data=df)\n",
    "    plt.title('Accidents by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# Day of week heatmap (hour vs day)\n",
    "if set(['hour','day_of_week']).issubset(df.columns):\n",
    "    pivot = df.pivot_table(index='hour', columns='day_of_week', values=df.columns[0], aggfunc='count').fillna(0)\n",
    "    # Reorder days\n",
    "    days_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "    pivot = pivot[days_order]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.heatmap(pivot, cmap='YlOrRd')\n",
    "    plt.title('Accident counts: Hour vs Day of Week')\n",
    "    plt.show()\n",
    "\n",
    "# Weather and road condition plots\n",
    "if 'weather_simple' in df.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    order = df['weather_simple'].value_counts().index\n",
    "    sns.countplot(y='weather_simple', data=df, order=order)\n",
    "    plt.title('Accidents by Weather Condition (simplified)')\n",
    "    plt.show()\n",
    "\n",
    "if 'road_simple' in df.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    order = df['road_simple'].value_counts().index\n",
    "    sns.countplot(y='road_simple', data=df, order=order)\n",
    "    plt.title('Accidents by Road Surface Condition (simplified)')\n",
    "    plt.show()\n",
    "\n",
    "# Severity analysis if available\n",
    "sev_cols = [c for c in df.columns if 'severity' in c.lower()]\n",
    "if sev_cols:\n",
    "    sc = sev_cols[0]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=sc, data=df)\n",
    "    plt.title('Accident Severity Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# Top cities/states (if columns exist)\n",
    "if 'City' in df.columns:\n",
    "    top_cities = df['City'].value_counts().head(10)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.barplot(x=top_cities.values, y=top_cities.index)\n",
    "    plt.title('Top 10 Cities by Accident Count')\n",
    "    plt.show()\n",
    "\n",
    "if 'State' in df.columns:\n",
    "    top_states = df['State'].value_counts().head(10)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.barplot(x=top_states.values, y=top_states.index)\n",
    "    plt.title('Top 10 States by Accident Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Hotspot visualization (folium) ===\n",
    "# Folium is optional; only run if lat/lng columns detected\n",
    "try:\n",
    "    import folium\n",
    "    from folium.plugins import HeatMap\n",
    "except Exception as e:\n",
    "    folium = None\n",
    "    print(\"Folium not installed; to enable map plotting, install folium in the environment.\")\n",
    "\n",
    "lat_col = [c for c in df.columns if 'lat' in c.lower()]\n",
    "lng_col = [c for c in df.columns if 'lon' in c.lower() or 'lng' in c.lower()]\n",
    "\n",
    "if lat_col and lng_col and folium is not None:\n",
    "    latc = lat_col[0]; lngc = lng_col[0]\n",
    "    # sample data for faster plotting\n",
    "    sample = df[[latc,lngc]].dropna().sample(min(len(df), 20000), random_state=42)\n",
    "    center = [sample[latc].mean(), sample[lngc].mean()]\n",
    "    m = folium.Map(location=center, zoom_start=6)\n",
    "    HeatMap(sample.values.tolist(), radius=8, blur=10).add_to(m)\n",
    "    display(m)\n",
    "    # Save map as HTML\n",
    "    m.save('accident_hotspot_map.html')\n",
    "    print(\"Saved hotspot map to accident_hotspot_map.html\")\n",
    "else:\n",
    "    print(\"Latitude/Longitude columns not found or folium not installed. Skipping hotspot map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Summary & Next Steps ===\n",
    "print(\"Accident data cleaning and EDA complete.\")\n",
    "print(\"Cleaned CSV: accidents_cleaned.csv\")\n",
    "print(\"If you want hotspot maps, ensure your dataset has latitude/longitude and install folium.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
